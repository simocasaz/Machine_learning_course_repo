{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Language Processing\n",
    "\n",
    "In this notebook, we explore text classification and entity recognition. \n",
    "\n",
    "We start by introducing the main architecture currently used to process texts: the Transformer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Transformer\n",
    "\n",
    "Published in the paper [Attention is All you Need](https://arxiv.org/abs/1706.03762) by Vaswani et al. (2017), the Transformer and its variants have become the main neural network architecture across data modalities. The Transformer is [natively implemented in PyTorch](https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html) as a network model.\n",
    "\n",
    "In our introduction to this architecture, we borrow from the following sources:\n",
    "1. [The Illustrated Transformer](https://jalammar.github.io/illustrated-transformer/).\n",
    "2. [A Very Gentle Introduction to Large Language Models without the Hype](https://mark-riedl.medium.com/a-very-gentle-introduction-to-large-language-models-without-the-hype-5f67941fa59e).\n",
    "3. [Tutorial 6: Transformers and Multi-Head Attention](https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/tutorial6/Transformers_and_MHAttention.html).\n",
    "4. [The Illustrated GPT2](https://jalammar.github.io/illustrated-gpt2/).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Language Models\n",
    "\n",
    "**Language models** allow us to represent textual inputs numerically, all the while storing information about all layers of the linguistic stack, from syntax to semantics, and more.\n",
    "\n",
    "Every language model starts with the need to represent text numerically. This step is often called **tokenization**: the slicing up of a text into units that are assigned a vector representation, or **embedding**, before being further processed by the language model. The most widely used tokenizers nowadays work at the subword level, i.e., short sequences of characters. Many different alternatives are possible.\n",
    "\n",
    "Once a text has been tokenized, and in order to train a language model, an approach is taken which is called **self-supervision**: large quantities of human-generated texts are used with words masked at random or sequentially, and the model is trained on the task of predicting the missing words. This simple approach turns out to be extremely powerful in practice. See [2] for more.\n",
    "\n",
    "Random masking:\n",
    "\n",
    "<img src=\"figures/mask1.png\" width=\"400px\" heigth=\"400px\">\n",
    "\n",
    "Sequential masking, or next word prediction:\n",
    "\n",
    "<img src=\"figures/mask2.png\" width=\"400px\" heigth=\"400px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"GPT stands for **Generative Pre-trained Transformer**. Let’s break this down:\n",
    "\n",
    "* Generative. The model is capable of generating continuations to the provided input. That is, given some text, the model tries to guess which words come next.\n",
    "* Pre-trained. The model is trained on a very large corpus of general text and is meant to be trained once and used for a lot of different things without needing to be re-trained from scratch.\n",
    "\n",
    "A **transformer** is a particular type of deep learning model that transforms the encoding in a particular way that makes it easier to guess the blanked out word. At the heart of a transformer is the classical **encoder-decoder network**. The encoder does a very standard encoding process, but then it adds something else called **self-attention**.\n",
    "\n",
    "A transformer learns which words in an input sequence are related and then creates a new encoding for each position in the input sequence that is a merger of all the related words.\" [2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/transformer_architecture.svg\" width=\"400px\" heigth=\"800px\">\n",
    "\n",
    "This figure illustrates a full encoder-decoder architecture, typically adopted for tasks where the full input needs to be accounted for when generating an output. An example is machine translation, or modality change (e.g., speech to text)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder-Decoder Architectures\n",
    "\n",
    "<img src=\"figures/encoderdecoder1.png\" width=\"400px\" heigth=\"400px\">\n",
    "\n",
    "The basic idea of Encoder-Decoder architectures originates by the need to deal with so called *\"sequence to sequence tasks\"*, e.g., translation. The input, for example your text in French, is first *encoded*, and then the output, for example a translation in English, is generated by the *decoder* on the basis of the encoded input. It turns out that these architectures are also generally useful when you want to generate an answer from a query, like ChatGPT does.\n",
    "\n",
    "<img src=\"figures/encoderdecoder3.png\" width=\"500px\" heigth=\"300px\">\n",
    "\n",
    "In the encoder and the decoder, we can use a variety of modules. \n",
    "\n",
    "With transformers, feed forward layers are used in conjunction with self-attention layers. When the encoded input is passed to the decoder, another form of attention is applied: cross-attention:\n",
    "\n",
    "<img src=\"figures/encoderdecoder2.png\" width=\"500px\" heigth=\"300px\">\n",
    "\n",
    "In cross-attention, the queries (*Q*) come from the decoder hidden states, while the keys (*K*) and values (*V*) come from the encoder hidden states. We will see next what this means."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention\n",
    "\n",
    "<img src=\"figures/attention1.png\" width=\"500px\" heigth=\"400px\">\n",
    "\n",
    "The key idea of attention is to allow the model to look at, or attend, at any relevant part of the input when making a prediction, for example when predicting a masked word, or translating a single word into another language.\n",
    "\n",
    "<img src=\"figures/attention2.png\" width=\"400px\" heigth=\"400px\">\n",
    "\n",
    "Attention units can be visualized and show how a model might consider different parts of the input at a given step, with varied intensity based on how relevant each part of the input is to the prediction at this step.\n",
    "\n",
    "#### Self-Attention\n",
    "\n",
    "Attention layers work by introducing three sets of parameters: *queries Q, keys K, and values V*.\n",
    "\n",
    "Following [1], we have that at each calculation of self-attention:\n",
    "\n",
    "1. The first step is to create three vectors from each of the encoder’s input vectors (in this case, the embedding of each word). So for each word, we create a query vector, a key vector, and a value vector. These vectors are created by multiplying the embedding by the three matrices Q, K, V, that are trained during the training process.\n",
    "2. The second step in calculating self-attention is to calculate a score. Say we’re calculating the self-attention for the first word in this example, “Thinking”. We need to score each word of the input sentence against this word. The score determines how much focus to place on other parts of the input sentence as we encode a word at a certain position.\n",
    "3. The third step applies a normalization and the softmax, to get probability scores for each input word against the current word.\n",
    "4. The next step is to multiply each value vector by the softmax score (in preparation to sum them up). The intuition here is to keep intact the values of the word(s) we want to focus on, and reduce the weight of irrelevant words.\n",
    "5. The fifth step is to sum up the weighted value vectors. This produces the output of the self-attention layer at this position (for the first word).\n",
    "\n",
    "<img src=\"figures/selfattention.png\" width=\"500px\" heigth=\"800px\">\n",
    "\n",
    "These operations are easily implemented as matrix multiplications. \n",
    "\n",
    "Note again that **cross-attention** works in a similar way, just the queries come from the decoder, whilst the keys and values from the encoder. Lastly, note that the transformer used **multi-head attention**: several attentio modules or heads are stacked up in each layer, the model training determine how they are actually used. This gives the model much more capacity.\n",
    "\n",
    "*See 6.2. How Does Self-Attention Work? [2] and [1] here for an in-depth discussion.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the transformer is a general purpose architecture. It has been used in encoder-only architectures (e.g., BERT: Bidirectional Encoder Representations from Transformers), decoder-only architectures (e.g., GPT), vision architectures (e.g., ViT: Vision Transformer), and more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also note that further tools used to improve GPTs after language model training:\n",
    "* **Instruction tuning**: basically, provide correct answers to the question you asked (i.e., supervised learning).\n",
    "* **Reinforcement learning from human feedback**: provide signal on whether the answer is good or bad (e.g., thumbs up or down). This is used as signal by the LLM to improve itself.\n",
    "\n",
    "See [4] for more details on these aspects.\n",
    "\n",
    "*This is a very quick introduction to the transformer, aiming only at providing intuition. In most cases, as we will see below, transformers are used as pre-trained models, that we then fine-tune or otherwise adjust to our specific tasks.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, DataCollatorWithPadding\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import pandas as pd\n",
    "from torch.optim import AdamW\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "## Imports for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from matplotlib.colors import to_rgba\n",
    "import seaborn as sns\n",
    "sns.set_theme('notebook', style='whitegrid')\n",
    "\n",
    "## Progress bar\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Set device (GPU or CPU)\n",
    "device = torch.device(\"mps\") if torch.backends.mps.is_available() else torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using torch 2.4.1+cu118\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "torch.manual_seed(42) # Setting the seed\n",
    "\n",
    "print(\"Using torch\", torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda\n",
      "tensor([1.], device='cuda:0')\n",
      "CUDA available: True\n",
      "CUDA version: 11.8\n",
      "Gpu check: completed\n"
     ]
    }
   ],
   "source": [
    "# Set device\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using {device}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    x = torch.ones(1, device=device)\n",
    "    print(x)\n",
    "    \n",
    "    # GPU operations have a separate seed we also want to set\n",
    "    torch.cuda.manual_seed(42)\n",
    "\n",
    "# Print CUDA availability and version\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"CUDA version:\", torch.version.cuda)\n",
    "\n",
    "print(\"Gpu check: completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text classification\n",
    "\n",
    "In this exercise, we are given a dataset of digitized books from the British Library. These books belong to different genres (e.g., poetry or prose). We are interested in training a classifier to distinguish between such genres. [Read more about this dataset here](https://github.com/mromanello/ADA-DHOxSS/tree/master/data#british-library-19th-century-books)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_books = pd.read_csv('data/bl_books/sample_tidy/df_book.csv')\n",
    "df_texts = pd.read_csv('data/bl_books/sample_tidy/df_book_text.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>datefield</th>\n",
       "      <th>publisher</th>\n",
       "      <th>title</th>\n",
       "      <th>edition</th>\n",
       "      <th>place</th>\n",
       "      <th>issuance</th>\n",
       "      <th>first_pdf</th>\n",
       "      <th>number_volumes</th>\n",
       "      <th>identifier</th>\n",
       "      <th>fulltext_filename</th>\n",
       "      <th>type</th>\n",
       "      <th>genre</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1841.0</td>\n",
       "      <td>Privately printed</td>\n",
       "      <td>The Poetical Aviary, with a bird's-eye view of...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Calcutta</td>\n",
       "      <td>monographic</td>\n",
       "      <td>lsidyv35c55757</td>\n",
       "      <td>1</td>\n",
       "      <td>196</td>\n",
       "      <td>000000196_01_text.json</td>\n",
       "      <td>poet</td>\n",
       "      <td>Poetry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1888.0</td>\n",
       "      <td>Rivingtons</td>\n",
       "      <td>A History of Greece. Part I. From the earliest...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>London</td>\n",
       "      <td>monographic</td>\n",
       "      <td>lsidyv376da437</td>\n",
       "      <td>1</td>\n",
       "      <td>4047</td>\n",
       "      <td>000004047_01_text.json</td>\n",
       "      <td>story</td>\n",
       "      <td>Prose</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   datefield          publisher  \\\n",
       "0     1841.0  Privately printed   \n",
       "1     1888.0         Rivingtons   \n",
       "\n",
       "                                               title edition     place  \\\n",
       "0  The Poetical Aviary, with a bird's-eye view of...     NaN  Calcutta   \n",
       "1  A History of Greece. Part I. From the earliest...     NaN    London   \n",
       "\n",
       "      issuance       first_pdf  number_volumes  identifier  \\\n",
       "0  monographic  lsidyv35c55757               1         196   \n",
       "1  monographic  lsidyv376da437               1        4047   \n",
       "\n",
       "        fulltext_filename   type   genre  \n",
       "0  000000196_01_text.json   poet  Poetry  \n",
       "1  000004047_01_text.json  story   Prose  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_books.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "genre\n",
       "Music     119\n",
       "Poetry    114\n",
       "Drama     111\n",
       "Prose     108\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_books.genre.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fulltext_filename</th>\n",
       "      <th>fulltext</th>\n",
       "      <th>book_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000551646_01_text.json</td>\n",
       "      <td>' -■\" ' LiLitr-- )Wm&amp;, HISTORY OF THE...</td>\n",
       "      <td>551646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>002674278_01_text.json</td>\n",
       "      <td>The Great Revolution of 1840. REMINISC...</td>\n",
       "      <td>2674278</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        fulltext_filename                                           fulltext  \\\n",
       "0  000551646_01_text.json           ' -■\" ' LiLitr-- )Wm&, HISTORY OF THE...   \n",
       "1  002674278_01_text.json          The Great Revolution of 1840. REMINISC...   \n",
       "\n",
       "   book_id  \n",
       "0   551646  \n",
       "1  2674278  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_texts.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_books.merge(df_texts, on='fulltext_filename')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>datefield</th>\n",
       "      <th>publisher</th>\n",
       "      <th>title</th>\n",
       "      <th>edition</th>\n",
       "      <th>place</th>\n",
       "      <th>issuance</th>\n",
       "      <th>first_pdf</th>\n",
       "      <th>number_volumes</th>\n",
       "      <th>identifier</th>\n",
       "      <th>fulltext_filename</th>\n",
       "      <th>type</th>\n",
       "      <th>genre</th>\n",
       "      <th>fulltext</th>\n",
       "      <th>book_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1841.0</td>\n",
       "      <td>Privately printed</td>\n",
       "      <td>The Poetical Aviary, with a bird's-eye view of...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Calcutta</td>\n",
       "      <td>monographic</td>\n",
       "      <td>lsidyv35c55757</td>\n",
       "      <td>1</td>\n",
       "      <td>196</td>\n",
       "      <td>000000196_01_text.json</td>\n",
       "      <td>poet</td>\n",
       "      <td>Poetry</td>\n",
       "      <td>THE POETICAL AVIARY, WITH A B I R D'S-E ...</td>\n",
       "      <td>196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1888.0</td>\n",
       "      <td>Rivingtons</td>\n",
       "      <td>A History of Greece. Part I. From the earliest...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>London</td>\n",
       "      <td>monographic</td>\n",
       "      <td>lsidyv376da437</td>\n",
       "      <td>1</td>\n",
       "      <td>4047</td>\n",
       "      <td>000004047_01_text.json</td>\n",
       "      <td>story</td>\n",
       "      <td>Prose</td>\n",
       "      <td>HISTORY OF GREECE ABBOTT  A HISTORY OF G...</td>\n",
       "      <td>4047</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   datefield          publisher  \\\n",
       "0     1841.0  Privately printed   \n",
       "1     1888.0         Rivingtons   \n",
       "\n",
       "                                               title edition     place  \\\n",
       "0  The Poetical Aviary, with a bird's-eye view of...     NaN  Calcutta   \n",
       "1  A History of Greece. Part I. From the earliest...     NaN    London   \n",
       "\n",
       "      issuance       first_pdf  number_volumes  identifier  \\\n",
       "0  monographic  lsidyv35c55757               1         196   \n",
       "1  monographic  lsidyv376da437               1        4047   \n",
       "\n",
       "        fulltext_filename   type   genre  \\\n",
       "0  000000196_01_text.json   poet  Poetry   \n",
       "1  000004047_01_text.json  story   Prose   \n",
       "\n",
       "                                            fulltext  book_id  \n",
       "0        THE POETICAL AVIARY, WITH A B I R D'S-E ...      196  \n",
       "1        HISTORY OF GREECE ABBOTT  A HISTORY OF G...     4047  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# divide all the data into training and testing\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24      Music\n",
       "17     Poetry\n",
       "66      Music\n",
       "301     Prose\n",
       "356     Drama\n",
       "        ...  \n",
       "106    Poetry\n",
       "270     Drama\n",
       "348     Drama\n",
       "435     Prose\n",
       "102     Drama\n",
       "Name: genre, Length: 361, dtype: object"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df[\"genre\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode string labels into integers using LabelEncoder from sklearn\n",
    "# The label column in both the training and testing dataframes contains string labels\n",
    "label_encoder = LabelEncoder()\n",
    "# Fit the encoder on the training labels and transform them to integers\n",
    "train_df['label'] = label_encoder.fit_transform(train_df['genre'])\n",
    "# Apply the same transformation on the test labels\n",
    "test_df['label'] = label_encoder.transform(test_df['genre'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.cuda.amp import GradScaler, autocast\n",
    "\n",
    "# Define a custom dataset class for our text data\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, max_len):\n",
    "        self.texts = dataframe['fulltext'].values  # Get the 'text' column from the dataframe\n",
    "        self.labels = dataframe['label'].values  # Get the 'label' column (which now contains integers)\n",
    "        self.tokenizer = tokenizer  # BERT tokenizer passed as an argument\n",
    "        self.max_len = max_len  # Maximum length for the tokenized input\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)  # Returns the number of samples in the dataset\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        text = self.texts[index]  # Get the text at the given index\n",
    "        label = self.labels[index]  # Get the label corresponding to the text\n",
    "\n",
    "        # Tokenize the text\n",
    "        # encode_plus returns a dictionary with input_ids, attention_mask, etc.\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,  # Add [CLS] and [SEP] tokens\n",
    "            max_length=self.max_len,  # Truncate or pad to the max length\n",
    "            return_token_type_ids=False,  # We don't need token type IDs for classification\n",
    "            padding='longest',  # Pad the sequence to max_len\n",
    "            return_attention_mask=True,  # Return attention mask (which indicates padded tokens)\n",
    "            return_tensors='pt',  # Return PyTorch tensors\n",
    "            truncation=True  # Truncate longer sequences\n",
    "        )\n",
    "\n",
    "        # Return the input ids, attention mask, and label for this sample\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)  # Convert label to tensor\n",
    "        }\n",
    "\n",
    "import torch\n",
    "\n",
    "def train_model(model, data_loader, optimizer, loss_fn, device, scheduler, n_examples, scaler, accumulation_steps=1):\n",
    "    model.train()  # Set model to training mode\n",
    "    total_loss = 0  # Initialize total loss\n",
    "    correct_predictions = 0  # Initialize correct predictions count\n",
    "\n",
    "    optimizer.zero_grad()  # Zero out gradients before starting accumulation\n",
    "\n",
    "    for step, batch in enumerate(data_loader):\n",
    "        # Move data to the device (GPU or CPU)\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        # Forward pass with mixed precision\n",
    "        with torch.amp.autocast(device_type='cuda', dtype=torch.float16):\n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                labels=labels\n",
    "            )\n",
    "            loss = outputs.loss / accumulation_steps  # Normalize loss for accumulation\n",
    "            logits = outputs.logits\n",
    "\n",
    "        # Backward pass (gradient accumulation)\n",
    "        scaler.scale(loss).backward()\n",
    "\n",
    "        # Perform optimization step only when accumulation_steps is reached\n",
    "        if (step + 1) % accumulation_steps == 0:\n",
    "            scaler.step(optimizer)  # Update model parameters\n",
    "            scaler.update()  # Update the scaler\n",
    "            optimizer.zero_grad()  # Zero out gradients\n",
    "            scheduler.step()  # Update learning rate\n",
    "\n",
    "        total_loss += loss.item() * accumulation_steps  # Accumulate total loss\n",
    "        _, preds = torch.max(logits, dim=1)  # Get predicted labels\n",
    "        correct_predictions += torch.sum(preds == labels)  # Count correct predictions\n",
    "\n",
    "    return correct_predictions.float() / n_examples, total_loss / len(data_loader)\n",
    "\n",
    "\n",
    "# Function to evaluate the model on the validation set\n",
    "def eval_model(model, data_loader, loss, device, n_examples):\n",
    "    model = model.eval()  # Set the model to evaluation mode\n",
    "    total_loss = 0  # Initialize the total loss\n",
    "    correct_predictions = 0  # Initialize correct predictions count\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient calculation (we're not training)\n",
    "        for batch in data_loader:\n",
    "            input_ids = batch['input_ids'].to(device)  # Move data to the device\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                labels=labels\n",
    "            )\n",
    "\n",
    "            loss = outputs.loss  # Get the loss\n",
    "            logits = outputs.logits  # Get the raw output (logits)\n",
    "\n",
    "            total_loss += loss.item()  # Accumulate the loss\n",
    "            _, preds = torch.max(logits, dim=1)  # Get the predicted labels\n",
    "            correct_predictions += torch.sum(preds == labels)  # Count correct predictions\n",
    "\n",
    "    return correct_predictions.float() / n_examples, total_loss / len(data_loader)  # Return accuracy and average loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\casaz\\AppData\\Local\\Temp\\ipykernel_18468\\3438089749.py:14: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler()  # Initialize the gradient scaler\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\casaz\\Documents\\Simone\\University\\dhdk\\machine_learning\\course_repository\\ml_project\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:440: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss 1.4127648395040762 accuracy 0.2742382287979126\n",
      "Test loss 1.3944446047147114 accuracy 0.20879121124744415\n",
      "Epoch 2/10\n",
      "----------\n",
      "Train loss 1.3894411532775215 accuracy 0.2825484573841095\n",
      "Test loss 1.4078588088353474 accuracy 0.21978022158145905\n",
      "Epoch 3/10\n",
      "----------\n",
      "Train loss 1.3673436278882234 accuracy 0.2853185534477234\n",
      "Test loss 1.3986765543619792 accuracy 0.21978022158145905\n",
      "Epoch 4/10\n",
      "----------\n",
      "Train loss 1.382015207539434 accuracy 0.2742382287979126\n",
      "Test loss 1.394800345102946 accuracy 0.21978022158145905\n",
      "Epoch 5/10\n",
      "----------\n",
      "Train loss 1.3708398808603701 accuracy 0.2908587157726288\n",
      "Test loss 1.3936499953269958 accuracy 0.21978022158145905\n",
      "Epoch 6/10\n",
      "----------\n",
      "Train loss 1.3725400178328804 accuracy 0.2963988780975342\n",
      "Test loss 1.3932254314422607 accuracy 0.21978022158145905\n",
      "Epoch 7/10\n",
      "----------\n",
      "Train loss 1.3685719966888428 accuracy 0.3019390404224396\n",
      "Test loss 1.3930674195289612 accuracy 0.21978022158145905\n",
      "Epoch 8/10\n",
      "----------\n",
      "Train loss 1.3759346941243047 accuracy 0.2936288118362427\n",
      "Test loss 1.3930305043856304 accuracy 0.21978022158145905\n",
      "Epoch 9/10\n",
      "----------\n",
      "Train loss 1.3767554241677988 accuracy 0.279778391122818\n",
      "Test loss 1.393021583557129 accuracy 0.21978022158145905\n",
      "Epoch 10/10\n",
      "----------\n",
      "Train loss 1.3683076578637827 accuracy 0.29916897416114807\n",
      "Test loss 1.3930199543635051 accuracy 0.21978022158145905\n"
     ]
    }
   ],
   "source": [
    "# Load pre-trained BERT tokenizer and model\n",
    "PRE_TRAINED_MODEL_NAME = 'bert-base-uncased'  # Using the uncased version of BERT\n",
    "tokenizer = BertTokenizer.from_pretrained(PRE_TRAINED_MODEL_NAME, clean_up_tokenization_spaces=True)  # Load tokenizer\n",
    "\n",
    "# Initialize a data collator for dynamic padding\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "# Hyperparameters\n",
    "FREEZE = False  # Whether to freeze the BERT layers or not\n",
    "MAX_LEN = 512  # Maximum length of input sequences\n",
    "BATCH_SIZE = 16  # Batch size for training and evaluation\n",
    "EPOCHS = 10  # Number of epochs to train the model\n",
    "LEARNING_RATE = 5e-5  # Learning rate for the optimizer\n",
    "scaler = GradScaler()  # Initialize the gradient scaler\n",
    "\n",
    "# Assume train_df and test_df are the pandas DataFrames containing the dataset\n",
    "# train_df = pd.DataFrame(...)  # DataFrame containing 'fulltext' and 'label'\n",
    "# test_df = pd.DataFrame(...)   # DataFrame containing 'fulltext' and 'label'\n",
    "\n",
    "# Create datasets for training and testing using the TextDataset class\n",
    "train_dataset = TextDataset(train_df, tokenizer, MAX_LEN)\n",
    "test_dataset = TextDataset(test_df, tokenizer, MAX_LEN)\n",
    "\n",
    "# Create DataLoaders to feed data into the model in batches\n",
    "train_data_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=data_collator)  # Shuffle the training data\n",
    "test_data_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, collate_fn=data_collator)  # No need to shuffle the test data\n",
    "\n",
    "# Initialize the pre-trained BERT model for sequence classification\n",
    "model = BertForSequenceClassification.from_pretrained(PRE_TRAINED_MODEL_NAME, num_labels=len(label_encoder.classes_))\n",
    "model = model.to(device)  # Move the model to the device (GPU/CPU)\n",
    "\n",
    "# Ensure all layers are trainable\n",
    "# NB this is costly in terms of memory and computation\n",
    "if not FREEZE:\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = True\n",
    "\n",
    "# Define the optimizer and learning rate scheduler\n",
    "optimizer = AdamW(model.parameters(), lr=LEARNING_RATE)  # AdamW optimizer (recommended for BERT)\n",
    "total_steps = len(train_data_loader) * EPOCHS  # Total number of training steps\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.9)  # Learning rate scheduler\n",
    "loss_fn = nn.CrossEntropyLoss().to(device)  # Cross entropy loss (common for classification tasks)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f'Epoch {epoch + 1}/{EPOCHS}')  # Print the current epoch\n",
    "    print('-' * 10)\n",
    "\n",
    "    # Train the model for one epoch\n",
    "    train_acc, train_loss = train_model(model, train_data_loader, optimizer, loss_fn, device, scheduler, len(train_df), scaler, accumulation_steps=2)\n",
    "    print(f'Train loss {train_loss} accuracy {train_acc}')  # Print training loss and accuracy\n",
    "\n",
    "    # Evaluate the model on the validation set\n",
    "    test_acc, test_loss = eval_model(model, test_data_loader, loss_fn, device, len(test_df))\n",
    "    print(f'Test loss {test_loss} accuracy {test_acc}')  # Print validation loss and accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Drama       1.00      0.00      0.00        24\n",
      "       Music       0.22      1.00      0.36        20\n",
      "      Poetry       1.00      0.00      0.00        26\n",
      "       Prose       1.00      0.00      0.00        21\n",
      "\n",
      "    accuracy                           0.22        91\n",
      "   macro avg       0.80      0.25      0.09        91\n",
      "weighted avg       0.83      0.22      0.08        91\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluation and metrics (after training is completed)\n",
    "y_true = []  # List to store true labels\n",
    "y_pred = []  # List to store predicted labels\n",
    "\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "with torch.no_grad():  # Disable gradient calculation (not needed for inference)\n",
    "    for batch in test_data_loader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)  # Forward pass\n",
    "        _, preds = torch.max(outputs.logits, dim=1)  # Get the predicted labels\n",
    "\n",
    "        y_true.extend(labels.cpu().numpy())  # Store true labels\n",
    "        y_pred.extend(preds.cpu().numpy())  # Store predicted labels\n",
    "\n",
    "# Print the classification report (precision, recall, F1-score)\n",
    "print(classification_report(y_true, y_pred, target_names=label_encoder.classes_, zero_division=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1 (Easy): RoBERTa\n",
    "\n",
    "The model clearly could use some improvement. One way it to switch to a better model architecture, which allows larger inputs to be processed. Consider RoBERTa:\n",
    "\n",
    "```Python\n",
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification\n",
    "\n",
    "# Change the model name to a RoBERTa pre-trained model\n",
    "PRE_TRAINED_MODEL_NAME = 'roberta-base'  # You can use 'roberta-large' for a larger model\n",
    "\n",
    "# Load the RoBERTa tokenizer and model\n",
    "tokenizer = RobertaTokenizer.from_pretrained(PRE_TRAINED_MODEL_NAME)\n",
    "model = RobertaForSequenceClassification.from_pretrained(PRE_TRAINED_MODEL_NAME, num_labels=len(label_encoder.classes_))\n",
    "\n",
    "# Move model to the appropriate device\n",
    "model = model.to(device)\n",
    "```\n",
    "\n",
    "You can remove the line: ```return_token_type_ids=False``` since RoBERTa uses a Byte-pair encoding that does not include type IDs. How is the performance changing? How about the training time?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7b4e14953d74629b2539b554c533d4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\casaz\\Documents\\Simone\\University\\dhdk\\machine_learning\\course_repository\\ml_project\\lib\\site-packages\\huggingface_hub\\file_download.py:142: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\casaz\\.cache\\huggingface\\hub\\models--roberta-base. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "089e6c0582894332b9d312dba7b145d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d97ed4551e25441f84706f412d3d558b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fea5d605c04e4b5daa03fbc598b12405",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a68dafba83cf41e4b79ce3d115684116",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/481 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c0198d9d84c4ce19a48eb5559f3b6d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/499M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Exercise one\n",
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification\n",
    "\n",
    "# Change the model name to a RoBERTa pre-trained model\n",
    "PRE_TRAINED_MODEL_NAME = 'roberta-base'  # You can use 'roberta-large' for a larger model\n",
    "\n",
    "# Load the RoBERTa tokenizer and model\n",
    "tokenizer = RobertaTokenizer.from_pretrained(PRE_TRAINED_MODEL_NAME)\n",
    "model = RobertaForSequenceClassification.from_pretrained(PRE_TRAINED_MODEL_NAME, num_labels=len(label_encoder.classes_))\n",
    "\n",
    "# Move model to the appropriate device\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modyfing the dataset class by removing token_type_ids\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, max_len):\n",
    "        self.texts = dataframe['fulltext'].values  # Get the 'text' column from the dataframe\n",
    "        self.labels = dataframe['label'].values  # Get the 'label' column (which now contains integers)\n",
    "        self.tokenizer = tokenizer  # BERT tokenizer passed as an argument\n",
    "        self.max_len = max_len  # Maximum length for the tokenized input\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)  # Returns the number of samples in the dataset\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        text = self.texts[index]  # Get the text at the given index\n",
    "        label = self.labels[index]  # Get the label corresponding to the text\n",
    "\n",
    "        # Tokenize the text\n",
    "        # encode_plus returns a dictionary with input_ids, attention_mask, etc.\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,  # Add [CLS] and [SEP] tokens\n",
    "            max_length=self.max_len,  # Truncate or pad to the max length\n",
    "            padding='longest',  # Pad the sequence to max_len\n",
    "            return_attention_mask=True,  # Return attention mask (which indicates padded tokens)\n",
    "            return_tensors='pt',  # Return PyTorch tensors\n",
    "            truncation=True  # Truncate longer sequences\n",
    "        )\n",
    "\n",
    "        # Return the input ids, attention mask, and label for this sample\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)  # Convert label to tensor\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "----------\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 48.00 MiB. GPU 0 has a total capacity of 6.00 GiB of which 0 bytes is free. Of the allocated memory 5.20 GiB is allocated by PyTorch, and 115.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 43\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m10\u001b[39m)\n\u001b[0;32m     42\u001b[0m \u001b[38;5;66;03m# Train the model for one epoch\u001b[39;00m\n\u001b[1;32m---> 43\u001b[0m train_acc, train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_data_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrain_df\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscaler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulation_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTrain loss \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m accuracy \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_acc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)  \u001b[38;5;66;03m# Print training loss and accuracy\u001b[39;00m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;66;03m# Evaluate the model on the validation set\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[14], line 55\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(model, data_loader, optimizer, loss_fn, device, scheduler, n_examples, scaler, accumulation_steps)\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;66;03m# Forward pass with mixed precision\u001b[39;00m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mamp\u001b[38;5;241m.\u001b[39mautocast(device_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat16):\n\u001b[1;32m---> 55\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     56\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     57\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     58\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\n\u001b[0;32m     59\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     60\u001b[0m     loss \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mloss \u001b[38;5;241m/\u001b[39m accumulation_steps  \u001b[38;5;66;03m# Normalize loss for accumulation\u001b[39;00m\n\u001b[0;32m     61\u001b[0m     logits \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mlogits\n",
      "File \u001b[1;32mc:\\Users\\casaz\\Documents\\Simone\\University\\dhdk\\machine_learning\\course_repository\\ml_project\\lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\casaz\\Documents\\Simone\\University\\dhdk\\machine_learning\\course_repository\\ml_project\\lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\casaz\\Documents\\Simone\\University\\dhdk\\machine_learning\\course_repository\\ml_project\\lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:1320\u001b[0m, in \u001b[0;36mRobertaForSequenceClassification.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1312\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1313\u001b[0m \u001b[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n\u001b[0;32m   1314\u001b[0m \u001b[38;5;124;03m    Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\u001b[39;00m\n\u001b[0;32m   1315\u001b[0m \u001b[38;5;124;03m    config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\u001b[39;00m\n\u001b[0;32m   1316\u001b[0m \u001b[38;5;124;03m    `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001b[39;00m\n\u001b[0;32m   1317\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1318\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m-> 1320\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mroberta\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1321\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1324\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1325\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1326\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1327\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1328\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1329\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1330\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1331\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   1332\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclassifier(sequence_output)\n",
      "File \u001b[1;32mc:\\Users\\casaz\\Documents\\Simone\\University\\dhdk\\machine_learning\\course_repository\\ml_project\\lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\casaz\\Documents\\Simone\\University\\dhdk\\machine_learning\\course_repository\\ml_project\\lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\casaz\\Documents\\Simone\\University\\dhdk\\machine_learning\\course_repository\\ml_project\\lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:976\u001b[0m, in \u001b[0;36mRobertaModel.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    969\u001b[0m \u001b[38;5;66;03m# Prepare head mask if needed\u001b[39;00m\n\u001b[0;32m    970\u001b[0m \u001b[38;5;66;03m# 1.0 in head_mask indicate we keep the head\u001b[39;00m\n\u001b[0;32m    971\u001b[0m \u001b[38;5;66;03m# attention_probs has shape bsz x n_heads x N x N\u001b[39;00m\n\u001b[0;32m    972\u001b[0m \u001b[38;5;66;03m# input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\u001b[39;00m\n\u001b[0;32m    973\u001b[0m \u001b[38;5;66;03m# and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\u001b[39;00m\n\u001b[0;32m    974\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[1;32m--> 976\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    977\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    978\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    979\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    980\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    981\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    982\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    983\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    984\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    985\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    986\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    987\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    988\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    989\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\casaz\\Documents\\Simone\\University\\dhdk\\machine_learning\\course_repository\\ml_project\\lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\casaz\\Documents\\Simone\\University\\dhdk\\machine_learning\\course_repository\\ml_project\\lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\casaz\\Documents\\Simone\\University\\dhdk\\machine_learning\\course_repository\\ml_project\\lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:631\u001b[0m, in \u001b[0;36mRobertaEncoder.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    620\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[0;32m    621\u001b[0m         layer_module\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[0;32m    622\u001b[0m         hidden_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    628\u001b[0m         output_attentions,\n\u001b[0;32m    629\u001b[0m     )\n\u001b[0;32m    630\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 631\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    632\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    633\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    634\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    635\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    636\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    637\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    638\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    639\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    641\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    642\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[1;32mc:\\Users\\casaz\\Documents\\Simone\\University\\dhdk\\machine_learning\\course_repository\\ml_project\\lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\casaz\\Documents\\Simone\\University\\dhdk\\machine_learning\\course_repository\\ml_project\\lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\casaz\\Documents\\Simone\\University\\dhdk\\machine_learning\\course_repository\\ml_project\\lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:562\u001b[0m, in \u001b[0;36mRobertaLayer.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    559\u001b[0m     cross_attn_present_key_value \u001b[38;5;241m=\u001b[39m cross_attention_outputs[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m    560\u001b[0m     present_key_value \u001b[38;5;241m=\u001b[39m present_key_value \u001b[38;5;241m+\u001b[39m cross_attn_present_key_value\n\u001b[1;32m--> 562\u001b[0m layer_output \u001b[38;5;241m=\u001b[39m \u001b[43mapply_chunking_to_forward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    563\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeed_forward_chunk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchunk_size_feed_forward\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mseq_len_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_output\u001b[49m\n\u001b[0;32m    564\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    565\u001b[0m outputs \u001b[38;5;241m=\u001b[39m (layer_output,) \u001b[38;5;241m+\u001b[39m outputs\n\u001b[0;32m    567\u001b[0m \u001b[38;5;66;03m# if decoder, return the attn key/values as the last output\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\casaz\\Documents\\Simone\\University\\dhdk\\machine_learning\\course_repository\\ml_project\\lib\\site-packages\\transformers\\pytorch_utils.py:261\u001b[0m, in \u001b[0;36mapply_chunking_to_forward\u001b[1;34m(forward_fn, chunk_size, chunk_dim, *input_tensors)\u001b[0m\n\u001b[0;32m    258\u001b[0m     \u001b[38;5;66;03m# concatenate output at same dimension\u001b[39;00m\n\u001b[0;32m    259\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcat(output_chunks, dim\u001b[38;5;241m=\u001b[39mchunk_dim)\n\u001b[1;32m--> 261\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minput_tensors\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\casaz\\Documents\\Simone\\University\\dhdk\\machine_learning\\course_repository\\ml_project\\lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:574\u001b[0m, in \u001b[0;36mRobertaLayer.feed_forward_chunk\u001b[1;34m(self, attention_output)\u001b[0m\n\u001b[0;32m    573\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfeed_forward_chunk\u001b[39m(\u001b[38;5;28mself\u001b[39m, attention_output):\n\u001b[1;32m--> 574\u001b[0m     intermediate_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mintermediate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattention_output\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    575\u001b[0m     layer_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput(intermediate_output, attention_output)\n\u001b[0;32m    576\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m layer_output\n",
      "File \u001b[1;32mc:\\Users\\casaz\\Documents\\Simone\\University\\dhdk\\machine_learning\\course_repository\\ml_project\\lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\casaz\\Documents\\Simone\\University\\dhdk\\machine_learning\\course_repository\\ml_project\\lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\casaz\\Documents\\Simone\\University\\dhdk\\machine_learning\\course_repository\\ml_project\\lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:473\u001b[0m, in \u001b[0;36mRobertaIntermediate.forward\u001b[1;34m(self, hidden_states)\u001b[0m\n\u001b[0;32m    471\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[0;32m    472\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdense(hidden_states)\n\u001b[1;32m--> 473\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mintermediate_act_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    474\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m hidden_states\n",
      "File \u001b[1;32mc:\\Users\\casaz\\Documents\\Simone\\University\\dhdk\\machine_learning\\course_repository\\ml_project\\lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\casaz\\Documents\\Simone\\University\\dhdk\\machine_learning\\course_repository\\ml_project\\lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\casaz\\Documents\\Simone\\University\\dhdk\\machine_learning\\course_repository\\ml_project\\lib\\site-packages\\transformers\\activations.py:78\u001b[0m, in \u001b[0;36mGELUActivation.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     77\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m---> 78\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mact\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 48.00 MiB. GPU 0 has a total capacity of 6.00 GiB of which 0 bytes is free. Of the allocated memory 5.20 GiB is allocated by PyTorch, and 115.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "# Initialize a data collator for dynamic padding\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "# Hyperparameters\n",
    "FREEZE = False  # Whether to freeze the BERT layers or not\n",
    "MAX_LEN = 512  # Maximum length of input sequences\n",
    "BATCH_SIZE = 16  # Batch size for training and evaluation\n",
    "EPOCHS = 10  # Number of epochs to train the model\n",
    "LEARNING_RATE = 5e-5  # Learning rate for the optimizer\n",
    "scaler = torch.amp.GradScaler(\"cuda\") # Initialize the gradient scaler\n",
    "\n",
    "# Assume train_df and test_df are the pandas DataFrames containing the dataset\n",
    "# train_df = pd.DataFrame(...)  # DataFrame containing 'fulltext' and 'label'\n",
    "# test_df = pd.DataFrame(...)   # DataFrame containing 'fulltext' and 'label'\n",
    "\n",
    "# Create datasets for training and testing using the TextDataset class\n",
    "train_dataset = TextDataset(train_df, tokenizer, MAX_LEN)\n",
    "test_dataset = TextDataset(test_df, tokenizer, MAX_LEN)\n",
    "\n",
    "# Create DataLoaders to feed data into the model in batches\n",
    "train_data_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=data_collator)  # Shuffle the training data\n",
    "test_data_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, collate_fn=data_collator)  # No need to shuffle the test data\n",
    "\n",
    "# Ensure all layers are trainable\n",
    "# NB this is costly in terms of memory and computation\n",
    "if not FREEZE:\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = True\n",
    "\n",
    "# Define the optimizer and learning rate scheduler\n",
    "optimizer = AdamW(model.parameters(), lr=LEARNING_RATE)  # AdamW optimizer (recommended for BERT)\n",
    "total_steps = len(train_data_loader) * EPOCHS  # Total number of training steps\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.9)  # Learning rate scheduler\n",
    "loss_fn = nn.CrossEntropyLoss().to(device)  # Cross entropy loss (common for classification tasks)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f'Epoch {epoch + 1}/{EPOCHS}')  # Print the current epoch\n",
    "    print('-' * 10)\n",
    "\n",
    "    # Train the model for one epoch\n",
    "    train_acc, train_loss = train_model(model, train_data_loader, optimizer, loss_fn, device, scheduler, len(train_df), scaler, accumulation_steps=2)\n",
    "    print(f'Train loss {train_loss} accuracy {train_acc}')  # Print training loss and accuracy\n",
    "\n",
    "    # Evaluate the model on the validation set\n",
    "    test_acc, test_loss = eval_model(model, test_data_loader, loss_fn, device, len(test_df))\n",
    "    print(f'Test loss {test_loss} accuracy {test_acc}')  # Print validation loss and accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2 (Medium): Book type\n",
    "\n",
    "Try using the `type` column instead of genre. Is it evenly distributed or not? How does the classifier perform with it?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Named Entity Recognition\n",
    "\n",
    "In this example, we are given a dataset of toponyms in 19th-century Engish newspapers. We are interested in developing a model to perform Named Entity Recognition, i.e., detecting mentions to entities of interest in a text. [The dataset is accessible here](https://github.com/hipe-eval/HIPE-2022-data/blob/main/documentation/README-topres19th.md#topres19th-dataset)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is how the data looks like:\n",
    "\n",
    "```\n",
    "#FORMAT=WebAnno TSV 3.2\n",
    "#T_SP=webanno.custom.Customentity|identifiier|value\n",
    "\n",
    "#Text=THE POOLE AND SOUTH-WESTERN HERALD, THURSDAY, OCTOBER 20, 1864.\n",
    "1-1\t0-3\tTHE\t_\t_\t\n",
    "1-2\t4-9\tPOOLE\t_\t_\t\n",
    "1-3\t10-13\tAND\t_\t_\t\n",
    "1-4\t14-27\tSOUTH-WESTERN\t_\t_\t\n",
    "1-5\t28-34\tHERALD\t_\t_\t\n",
    "1-6\t34-35\t,\t_\t_\t\n",
    "1-7\t36-44\tTHURSDAY\t_\t_\t\n",
    "1-8\t44-45\t,\t_\t_\t\n",
    "1-9\t46-53\tOCTOBER\t_\t_\t\n",
    "1-10\t54-56\t20\t_\t_\t\n",
    "1-11\t56-57\t,\t_\t_\t\n",
    "1-12\t58-62\t1864\t_\t_\t\n",
    "1-13\t62-63\t.\t_\t_\t\n",
    "\n",
    "#Text=POOLE TOWN COUNCIL.\n",
    "2-1\t65-70\tPOOLE\thttps://en.wikipedia.org/wiki/Poole\tLOC\n",
    "2-2\t71-75\tTOWN\t_\t_\t\n",
    "2-3\t76-83\tCOUNCIL\t_\t_\t\n",
    "2-4\t83-84\t.\t_\t_\t\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, we need to load data into memory\n",
    "\n",
    "import os, re, random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Path to the dataset directory\n",
    "DATA_DIR = \"data/topRes19th_v2\"\n",
    "\n",
    "allowed_classes = ['LOC','_']\n",
    "# Create the label_to_id dictionary by enumerating the allowed_classes\n",
    "label_to_id = {label: idx for idx, label in enumerate(allowed_classes)}\n",
    "def strip_square_brackets(word):\n",
    "    # Use regex to remove brackets and their contents\n",
    "    return re.sub(r'\\[.*?\\]', '', word)\n",
    "\n",
    "class NERDataset(Dataset):\n",
    "    def __init__(self, data_dir, split=\"train\", context_window=3):\n",
    "        \"\"\"\n",
    "        Initialize the dataset by loading all the files in the specified split (train/test).\n",
    "        Strips out long sequences of non-entity tokens (labeled '_'), and keeps tokens \n",
    "        with 'LOC' labels along with a context window of surrounding tokens.\n",
    "\n",
    "        Args:\n",
    "        - data_dir (str): The directory where the dataset is stored.\n",
    "        - split (str): 'train' or 'test'.\n",
    "        - context_window (int): The number of surrounding tokens to keep around LOC occurrences.\n",
    "        \"\"\"\n",
    "        self.data_dir = os.path.join(data_dir, split, \"annotated_tsv\")\n",
    "        self.files = [f for f in os.listdir(self.data_dir) if f.endswith(\".tsv\")]\n",
    "        self.context_window = context_window  # Context window around LOC tokens\n",
    "        self.data = []\n",
    "        self._load_data()\n",
    "\n",
    "    def _load_data(self):\n",
    "        \"\"\" Load and parse the data from the TSV files. \"\"\"\n",
    "        for file in self.files:\n",
    "            file_path = os.path.join(self.data_dir, file)\n",
    "            with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                tokens, labels = [], []\n",
    "                for line in f:\n",
    "                    if line.strip() == \"\" or line.startswith(\"#\"):  # Skip headers and empty lines\n",
    "                        continue\n",
    "                    columns = line.strip().split(\"\\t\")\n",
    "                    token = columns[2]  # The word/token itself\n",
    "                    label = strip_square_brackets(columns[-1])  # The NER label (LOC, BUILDINGS, etc.)\n",
    "                    if label != \"_\":\n",
    "                        label = \"LOC\"  # Simplify the label to LOC\n",
    "                    tokens.append(token)\n",
    "                    labels.append(label)\n",
    "\n",
    "                # Now we strip the excess '_' tokens and keep the context around 'LOC' tokens\n",
    "                stripped_tokens, stripped_labels = self._strip_non_loc(tokens, labels)\n",
    "\n",
    "                if stripped_tokens:\n",
    "                    self.data.append((stripped_tokens, stripped_labels))\n",
    "\n",
    "    def _strip_non_loc(self, tokens, labels):\n",
    "        \"\"\"\n",
    "        Strips long sequences of tokens labeled as '_', keeping tokens with 'LOC' labels and\n",
    "        a context window of surrounding tokens.\n",
    "\n",
    "        Args:\n",
    "        - tokens (List[str]): List of tokens in a sentence.\n",
    "        - labels (List[str]): List of corresponding labels.\n",
    "\n",
    "        Returns:\n",
    "        - stripped_tokens (List[str]): List of tokens after stripping excess '_'.\n",
    "        - stripped_labels (List[str]): List of labels after stripping excess '_'.\n",
    "        \"\"\"\n",
    "        stripped_tokens = []\n",
    "        stripped_labels = []\n",
    "\n",
    "        loc_indices = [i for i, label in enumerate(labels) if label == \"LOC\"]\n",
    "\n",
    "        if not loc_indices:\n",
    "            # No LOC labels, return empty (this sentence will be dropped)\n",
    "            return stripped_tokens, stripped_labels\n",
    "\n",
    "        # Loop through each LOC token and capture its context\n",
    "        for loc_idx in loc_indices:\n",
    "            # Start and end indices for the context window around LOC\n",
    "            start_idx = max(0, loc_idx - self.context_window)\n",
    "            end_idx = min(len(tokens), loc_idx + self.context_window + 1)\n",
    "\n",
    "            # Append the tokens and labels in the context window\n",
    "            stripped_tokens.extend(tokens[start_idx:end_idx])\n",
    "            stripped_labels.extend(labels[start_idx:end_idx])\n",
    "\n",
    "        return stripped_tokens, stripped_labels\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\" Returns the total number of articles in the dataset. \"\"\"\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Returns the tokens and labels for the given index.\n",
    "        Args:\n",
    "        - idx (int): The index of the article.\n",
    "        Returns:\n",
    "        - tokens (List[str]): The list of tokens.\n",
    "        - labels (List[str]): The corresponding NER labels for the tokens.\n",
    "        \"\"\"\n",
    "        tokens, labels = self.data[idx]\n",
    "        return tokens, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "# Load the dataset\n",
    "train_dataset = NERDataset(DATA_DIR, split=\"train\")\n",
    "test_dataset = NERDataset(DATA_DIR, split=\"test\")\n",
    "\n",
    "# Example: Load one article\n",
    "tokens, labels = train_dataset[0]\n",
    "print(\"Tokens:\", tokens)\n",
    "print(\"Labels:\", labels)\n",
    "\n",
    "# PyTorch DataLoader\n",
    "train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
    "\n",
    "# Iterate through the DataLoader\n",
    "for batch in train_loader:\n",
    "    tokens, labels = batch\n",
    "    print(\"Batch tokens:\", tokens)\n",
    "    print(\"Batch labels:\", labels)\n",
    "    break  # Stop after the first batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import torch\n",
    "\n",
    "def calculate_class_weights(train_dataset, label_to_id):\n",
    "    label_counts = Counter()\n",
    "    \n",
    "    for example in train_dataset:\n",
    "        labels = list(map(lambda x: x, example[1]))  # Extract the label from tuple\n",
    "        label_counts.update(labels)\n",
    "    \n",
    "    total_labels = sum(label_counts.values())\n",
    "    class_weights = {}\n",
    "    print(label_counts)\n",
    "    \n",
    "    # Inverse weighting: More frequent classes get lower weights\n",
    "    for label, count in label_counts.items():\n",
    "        class_weights[label_to_id[label]] = total_labels / (len(label_to_id) * count)\n",
    "    \n",
    "    # Convert to tensor\n",
    "    class_weights = torch.tensor([class_weights[i] for i in range(len(label_to_id))], dtype=torch.float)\n",
    "    \n",
    "    return class_weights\n",
    "\n",
    "# Example: Calculate class weights\n",
    "class_weights = calculate_class_weights(train_dataset, label_to_id)\n",
    "print(f\"Class Weights: {class_weights}\")\n",
    "print(label_to_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch, tokenizer, label_to_id, max_length=512, label_pad_token=-100):\n",
    "    \"\"\"\n",
    "    Collate function to process batches with tokens and labels stored in tuples.\n",
    "    \n",
    "    Args:\n",
    "    - batch: List of dicts containing 'tokens' and 'labels', where both are tuples.\n",
    "    - tokenizer: Pretrained tokenizer (RobertaTokenizer).\n",
    "    - label_to_id: Dictionary mapping string labels (e.g., 'LOC') to integers.\n",
    "    - max_length: Maximum sequence length (default 512 for RoBERTa).\n",
    "    - label_pad_token: Token to pad labels, default is -100.\n",
    "    \n",
    "    Returns:\n",
    "    - input_ids: Tensor of tokenized input sequences.\n",
    "    - labels: Tensor of padded labels.\n",
    "    - attention_mask: Tensor mask to ignore padding tokens.\n",
    "    \"\"\"\n",
    "    # Extract tokens and labels from tuples\n",
    "    tokens = [list(map(lambda x: x, item[0])) for item in batch]\n",
    "    labels = [list(map(lambda x: x, item[1])) for item in batch]\n",
    "\n",
    "    # Tokenize the tokens with truncation and padding\n",
    "    tokenized_inputs = tokenizer(tokens, \n",
    "                                 is_split_into_words=True, \n",
    "                                 padding=True, \n",
    "                                 truncation=True, \n",
    "                                 max_length=max_length,  # Ensure max length, also drops what is longer!\n",
    "                                 return_tensors=\"pt\")\n",
    "    \n",
    "    # Map string labels to integers and pad them to match input length\n",
    "    max_len = tokenized_inputs['input_ids'].shape[1]  # Get max length from tokenized input\n",
    "    padded_labels = []\n",
    "    for label_seq in labels:\n",
    "        # Convert string labels to IDs\n",
    "        label_ids = [label_to_id.get(label, label_pad_token) for label in label_seq]\n",
    "        padded_label = label_ids[:max_len] + [label_pad_token] * (max_len - len(label_ids))  # Truncate and pad\n",
    "        padded_labels.append(padded_label)\n",
    "    \n",
    "    # Convert to tensors\n",
    "    padded_labels = torch.tensor(padded_labels, dtype=torch.long)\n",
    "    \n",
    "    return tokenized_inputs['input_ids'], padded_labels, tokenized_inputs['attention_mask']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.optim import AdamW\n",
    "from transformers import RobertaTokenizer, RobertaForTokenClassification\n",
    "\n",
    "# Load pre-trained RoBERTa model and tokenizer\n",
    "model = RobertaForTokenClassification.from_pretrained('roberta-base', num_labels=len(allowed_classes))\n",
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base', clean_up_tokenization_spaces=True)\n",
    "\n",
    "BATCH_SIZE = 50  # Adjust batch size as necessary\n",
    "EPOCHS = 10  # Adjust epochs as necessary\n",
    "\n",
    "# Create PyTorch datasets and loaders\n",
    "train_dataset = NERDataset(DATA_DIR, split=\"train\")\n",
    "test_dataset = NERDataset(DATA_DIR, split=\"test\")\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, collate_fn=lambda batch: collate_fn(batch, tokenizer, label_to_id))\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, collate_fn=lambda batch: collate_fn(batch, tokenizer, label_to_id))\n",
    "\n",
    "model.to(device)  # Move model to device\n",
    "\n",
    "# Define the weighted loss function\n",
    "loss_fn = torch.nn.CrossEntropyLoss(weight=class_weights.to(device), ignore_index=-100)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "# Ensure all layers are trainable\n",
    "# NB this is costly in terms of memory and computation\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "# Training loop with device placement\n",
    "model.train()\n",
    "for epoch in range(EPOCHS):  # Adjust epochs as necessary\n",
    "    for tokens, labels, mask in train_loader:\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Move data to device\n",
    "        tokens = tokens.to(device)\n",
    "        labels = labels.to(device)\n",
    "        mask = mask.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(input_ids=tokens, attention_mask=mask)\n",
    "        \n",
    "        # Compute loss with weighted CrossEntropyLoss\n",
    "        loss = loss_fn(outputs.logits.view(-1, len(label_to_id)), labels.view(-1))\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f'Epoch {epoch + 1}, Loss: {loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import numpy as np\n",
    "\n",
    "# Prepare lists to accumulate predictions and true labels across batches\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "# Evaluation on the test set with device placement\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for tokens, labels, mask in test_loader:\n",
    "        # Move data to device\n",
    "        tokens = tokens.to(device)\n",
    "        labels = labels.to(device)\n",
    "        mask = mask.to(device)\n",
    "\n",
    "        # Get model predictions\n",
    "        outputs = model(input_ids=tokens, attention_mask=mask)\n",
    "        predictions = torch.argmax(outputs.logits, dim=-1)\n",
    "\n",
    "        # Convert predictions and labels back to CPU and flatten the arrays\n",
    "        all_preds.append(predictions.cpu().numpy().flatten())\n",
    "        all_labels.append(labels.cpu().numpy().flatten())\n",
    "\n",
    "# Concatenate all batches into a single array\n",
    "all_preds = np.concatenate(all_preds)\n",
    "all_labels = np.concatenate(all_labels)\n",
    "\n",
    "# Filter out padding tokens (ignore -100 in labels)\n",
    "mask = all_labels != -100\n",
    "all_preds = all_preds[mask]\n",
    "all_labels = all_labels[mask]\n",
    "\n",
    "# Get the class names (in the same order as `label_to_id`)\n",
    "class_names = [x[0] for x in sorted(label_to_id.items(), key=lambda item: item[1])]\n",
    "\n",
    "# Confusion Matrix\n",
    "conf_matrix = confusion_matrix(all_labels, all_preds)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "\n",
    "# Classification Report with micro and macro averages\n",
    "report = classification_report(all_labels, all_preds, target_names=class_names, zero_division=0)\n",
    "print(\"\\nClassification Report:\")\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1 (Easy)\n",
    "\n",
    "The results are not outstanding, but there is a lot we can tweak and try out to make them better. Try to change pretrained model, configure hyperparameters differently, and see if you can improve upon results.\n",
    "\n",
    "This model is intense to train. If you experience excessive delays, attempt to reduce the data you use to train it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2 (Easy)\n",
    "\n",
    "Experiment with the context window parameter. What happens when you reduce it or when you increase it?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3 (Medium)\n",
    "\n",
    "The original dataset contains more classes than `LOC`. Check them and see how frequent they are. Second, keep them into the dataset and train the model again using multiple classes. How does the model performance changes?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
